{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2810b491-9a16-4ec3-beac-637481dabfc9",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "## OOD classification 테스크를 위한 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0201ff83-6c4e-463c-8541-7d60093ca88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "from src.dataset import Preprocessing\n",
    "from src.model import EpochLogger, MakeEmbed, save\n",
    "\n",
    "class MakeDataset:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.intent_ood_label_dir = \"./data/dataset/intent_label_with_ood.json\"\n",
    "        self.intent_data_dir = \"./data/dataset/intent_data.csv\"\n",
    "        self.ood_data_dir = \"./data/dataset/ood_data.csv\"\n",
    "        \n",
    "        self.intent_ood_label = self.load_intent_ood_label()\n",
    "        self.prep = Preprocessing()\n",
    "    \n",
    "    def load_intent_ood_label(self):\n",
    "        f = open(self.intent_ood_label_dir, encoding=\"UTF-8\")\n",
    "        intent_ood_label = json.loads(f.read())\n",
    "        self.intents_ood = list(intent_ood_label.keys())\n",
    "        return intent_ood_label\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        return sentence.split()\n",
    "    \n",
    "    def tokenize_dataset(self, dataset):\n",
    "        token_dataset = []\n",
    "        for data in dataset:\n",
    "            token_dataset.append(self.tokenize(data))\n",
    "        return token_dataset\n",
    "\n",
    "    def make_ood_dataset(self, embed):\n",
    "        intent_dataset = pd.read_csv(self.intent_data_dir)\n",
    "        ood_dataset = pd.read_csv(self.ood_data_dir)#.sample(frac=1).reset_index(drop=True)\n",
    "        intent_dataset = pd.concat([intent_dataset,ood_dataset])\n",
    "        labels = []\n",
    "        for label in intent_dataset[\"label\"].to_list():\n",
    "            if(label == \"OOD\"):\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "            \n",
    "        intent_querys = self.tokenize_dataset(intent_dataset[\"question\"].tolist())\n",
    "        \n",
    "        dataset = list(zip(intent_querys, labels))\n",
    "        intent_train_dataset, intent_test_dataset = self.word2idx_dataset(dataset, embed)\n",
    "        return intent_train_dataset, intent_test_dataset\n",
    "    \n",
    "    \n",
    "    def word2idx_dataset(self, dataset ,embed, train_ratio = 0.8):\n",
    "        embed_dataset = []\n",
    "        question_list, label_list = [], []\n",
    "        flag = True\n",
    "        random.shuffle(dataset)\n",
    "        for query, label in dataset :\n",
    "            q_vec = embed.query2idx(query)\n",
    "            q_vec = self.prep.pad_idx_sequencing(q_vec)\n",
    "\n",
    "            question_list.append(torch.tensor([q_vec]))\n",
    "\n",
    "            label_list.append(torch.tensor([label]))\n",
    "\n",
    "        x = torch.cat(question_list)\n",
    "        y = torch.cat(label_list)\n",
    "\n",
    "        x_len = x.size()[0]\n",
    "        y_len = y.size()[0]\n",
    "        if(x_len == y_len):\n",
    "            train_size = int(x_len*train_ratio)\n",
    "            \n",
    "            train_x = x[:train_size]\n",
    "            train_y = y[:train_size]\n",
    "\n",
    "            test_x = x[train_size+1:]\n",
    "            test_y = y[train_size+1:]\n",
    "            \n",
    "            train_dataset = TensorDataset(train_x,train_y)\n",
    "            test_dataset = TensorDataset(test_x,test_y)\n",
    "            \n",
    "            return train_dataset, test_dataset\n",
    "            \n",
    "        else:\n",
    "            print(\"ERROR x!=y\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc4bb28e-d938-4694-82e1-9d0f10d0873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MakeDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9af5b413-51f5-4798-81a7-9451773c5426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>저기 그림 화면 그릴 거야</td>\n",
       "      <td>OOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그림 틀어줄래</td>\n",
       "      <td>OOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>저기 있잖아 그림 좀</td>\n",
       "      <td>OOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그림 그릴 거야</td>\n",
       "      <td>OOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>그림 켜줄래</td>\n",
       "      <td>OOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12218</th>\n",
       "      <td>훔쳐보는 것 눈치 보임</td>\n",
       "      <td>OOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12219</th>\n",
       "      <td>훔쳐보는 것 눈치 보임</td>\n",
       "      <td>OOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12220</th>\n",
       "      <td>흑기사 해주는 짝남</td>\n",
       "      <td>OOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12221</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까</td>\n",
       "      <td>OOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12222</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>OOD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12223 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     question label\n",
       "0              저기 그림 화면 그릴 거야   OOD\n",
       "1                     그림 틀어줄래   OOD\n",
       "2                 저기 있잖아 그림 좀   OOD\n",
       "3                    그림 그릴 거야   OOD\n",
       "4                      그림 켜줄래   OOD\n",
       "...                       ...   ...\n",
       "12218            훔쳐보는 것 눈치 보임   OOD\n",
       "12219            훔쳐보는 것 눈치 보임   OOD\n",
       "12220              흑기사 해주는 짝남   OOD\n",
       "12221  힘든 연애 좋은 연애라는게 무슨 차이일까   OOD\n",
       "12222              힘들어서 결혼할까봐   OOD\n",
       "\n",
       "[12223 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(dataset.ood_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4f64443-d6d4-48df-abcb-ffca0467b3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>야 먼지 알려주겠니</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아니 먼지 정보 알려주세요</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그 때 미세먼지 어떨까</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그 때 먼지 좋으려나</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미세먼지 어떨 것 같은데</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>구미 날씨 덥니</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>영암 우산 가져갈까</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>군포 비오냐</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>하남 덥냐</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>순천 우산 필요하니</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19992 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             question    label\n",
       "0          야 먼지 알려주겠니     dust\n",
       "1      아니 먼지 정보 알려주세요     dust\n",
       "2        그 때 미세먼지 어떨까     dust\n",
       "3         그 때 먼지 좋으려나     dust\n",
       "4       미세먼지 어떨 것 같은데     dust\n",
       "...               ...      ...\n",
       "19987        구미 날씨 덥니  weather\n",
       "19988      영암 우산 가져갈까  weather\n",
       "19989          군포 비오냐  weather\n",
       "19990           하남 덥냐  weather\n",
       "19991      순천 우산 필요하니  weather\n",
       "\n",
       "[19992 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(dataset.intent_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0b57b18-d62d-4a2b-af7d-cd1b7b9dd1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = MakeEmbed()\n",
    "embed.load_word2vec()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ood_train_dataset, ood_test_dataset = dataset.make_ood_dataset(embed)\n",
    "\n",
    "train_dataloader = DataLoader(ood_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(ood_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816fe16-f39d-4bfd-9489-49e941b57d66",
   "metadata": {},
   "source": [
    "# Deep Unordered Composition Rivals Syntactic Methods for Text Classification\n",
    "## * Iyyer\n",
    "### tensorflow code(tf-hub로 제공) : https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "### keras code : https://github.com/candlewill/Vecamend-master2/blob/master/dan.py#L41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b23b168-8ce6-41c8-82be-5a881c2855c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "    \n",
    "    def __init__(self, w2v, dim, dropout, num_class = 2):\n",
    "        super(DAN, self).__init__()\n",
    "        #load pretrained embedding in embedding layer.\n",
    "        vocab_size = w2v.size()[0]\n",
    "        emb_dim = w2v.size()[1]\n",
    "        self.embed = nn.Embedding(vocab_size+2, emb_dim)\n",
    "        self.embed.weight[2:].data.copy_(w2v)\n",
    "        #self.embed.weight.requires_grad = False\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.bn1 = nn.BatchNorm1d(emb_dim)\n",
    "        self.fc1 = nn.Linear(emb_dim, dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.bn2 = nn.BatchNorm1d(dim)\n",
    "        self.fc2 = nn.Linear(dim, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb_x = self.embed(x)\n",
    "        x = emb_x.mean(dim=1)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.bn2(x)\n",
    "        logit = self.fc2(x)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da547b28-a38f-485c-b5b4-b8fb81d6a9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAN(\n",
       "  (embed): Embedding(1481, 300)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (bn1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=300, out_features=256, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = embed.word2vec.wv.vectors\n",
    "weights = torch.FloatTensor(weights)\n",
    "\n",
    "model = DAN(weights, 256, 0.5, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd0377-25ea-431d-8fe8-0143eaa456f2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef374fcf-1604-4d0d-a7a3-66fdef78b601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 202/202 [00:04<00:00, 43.66batch/s, accuracy=100.0, loss=0.00749]  \n",
      "Epoch 0: 100%|██████████| 51/51 [00:00<00:00, 211.62batch/s, accuracy=99.2, loss=0.0501] \n",
      "Epoch 1: 100%|██████████| 202/202 [00:04<00:00, 44.91batch/s, accuracy=100.0, loss=0.0116]   \n",
      "Epoch 1: 100%|██████████| 51/51 [00:00<00:00, 231.82batch/s, accuracy=99.3, loss=0.00242]\n",
      "Epoch 2: 100%|██████████| 202/202 [00:04<00:00, 45.99batch/s, accuracy=100.0, loss=0.00511]   \n",
      "Epoch 2: 100%|██████████| 51/51 [00:00<00:00, 229.73batch/s, accuracy=99.5, loss=0.00578] \n",
      "Epoch 3: 100%|██████████| 202/202 [00:04<00:00, 45.43batch/s, accuracy=100.0, loss=0.000901]  \n",
      "Epoch 3: 100%|██████████| 51/51 [00:00<00:00, 168.88batch/s, accuracy=99.5, loss=0.00145] \n",
      "Epoch 4: 100%|██████████| 202/202 [00:04<00:00, 41.69batch/s, accuracy=100.0, loss=0.00442]   \n",
      "Epoch 4: 100%|██████████| 51/51 [00:00<00:00, 207.32batch/s, accuracy=99.7, loss=0.000615]\n",
      "Epoch 5: 100%|██████████| 202/202 [00:04<00:00, 43.99batch/s, accuracy=100.0, loss=0.000277]  \n",
      "Epoch 5: 100%|██████████| 51/51 [00:00<00:00, 209.02batch/s, accuracy=99.7, loss=0.00026] \n",
      "Epoch 6: 100%|██████████| 202/202 [00:04<00:00, 45.72batch/s, accuracy=95.454544, loss=0.152] \n",
      "Epoch 6: 100%|██████████| 51/51 [00:00<00:00, 221.74batch/s, accuracy=99.7, loss=0.000237]\n",
      "Epoch 7: 100%|██████████| 202/202 [00:04<00:00, 44.39batch/s, accuracy=100.0, loss=0.00123]   \n",
      "Epoch 7: 100%|██████████| 51/51 [00:00<00:00, 220.78batch/s, accuracy=99.7, loss=8.26e-5] \n",
      "Epoch 8: 100%|██████████| 202/202 [00:04<00:00, 45.85batch/s, accuracy=95.454544, loss=0.136] \n",
      "Epoch 8: 100%|██████████| 51/51 [00:00<00:00, 212.50batch/s, accuracy=99.8, loss=0.000524]\n",
      "Epoch 9: 100%|██████████| 202/202 [00:04<00:00, 46.25batch/s, accuracy=100.0, loss=0.0012]    \n",
      "Epoch 9: 100%|██████████| 51/51 [00:00<00:00, 222.71batch/s, accuracy=99.7, loss=0.000566]\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "prev_acc = 0\n",
    "save_dir = \"./data/pretraining/1_ood_clsf_model//\"\n",
    "save_prefix = \"ood_clsf\"\n",
    "for i in range(epoch):\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    #for data in train_dataloader:\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            target = data[1]\n",
    "            logit = model.forward(x)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(logit, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            accuracy = 100.0 * corrects/x.size()[0]\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy= accuracy.numpy())\n",
    "            \n",
    "    model.eval()\n",
    "    steps = 0\n",
    "    accuarcy_list = []\n",
    "    #for data in test_dataloader:\n",
    "    with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            target = data[1]\n",
    "\n",
    "            logit = model.forward(x)\n",
    "            loss = F.cross_entropy(logit, target)\n",
    "            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            accuracy = 100.0 * corrects/x.size()[0]\n",
    "            accuarcy_list.append(accuracy.tolist())\n",
    "            \n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy= sum(accuarcy_list)/len(accuarcy_list))\n",
    "            \n",
    "    acc = sum(accuarcy_list)/len(accuarcy_list)\n",
    "    if(acc>prev_acc):\n",
    "        prev_acc = acc\n",
    "        save(model, save_dir, save_prefix+\"_\"+str(round(acc,3)), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e732a5-b1db-45ce-b393-5fb3230aae8b",
   "metadata": {},
   "source": [
    "# Load & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b22dbb1-e8a8-4909-845e-c54d68161760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAN(\n",
       "  (embed): Embedding(1481, 300)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (bn1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=300, out_features=256, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./data/pretraining/1_ood_clsf_model/ood_clsf_99.786_steps_8.pt\"))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38734464-2d56-4f12-a66a-3d77d1619be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발화 : 제주도\n",
      "ood\n"
     ]
    }
   ],
   "source": [
    "q = \"제주도\"\n",
    "\n",
    "x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n",
    "\n",
    "x = torch.tensor(x)\n",
    "f = model(x.unsqueeze(0))\n",
    "\n",
    "y = torch.argmax(f).tolist()\n",
    "\n",
    "print(\"발화 : \" + q)\n",
    "if(not y):\n",
    "    print(\"ood\")\n",
    "else:\n",
    "    print(\"intent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7484825c-33c6-4b66-9cc5-4672fe2e6a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발화 : 제주도 날씨\n",
      "intent\n"
     ]
    }
   ],
   "source": [
    "q = \"제주도 날씨\"\n",
    "\n",
    "x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n",
    "\n",
    "x = torch.tensor(x)\n",
    "f = model(x.unsqueeze(0))\n",
    "\n",
    "y = torch.argmax(f).tolist()\n",
    "\n",
    "print(\"발화 : \" + q)\n",
    "if(not y):\n",
    "    print(\"ood\")\n",
    "else:\n",
    "    print(\"intent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e07491b-ca67-449c-9697-6528cb98b868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
